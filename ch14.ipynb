{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14장 모델의 성능 향상시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://raw.githubusercontent.com/taehojo/taehojo.github.io/master/assets/images/linktocolab.png\" align=\"left\"/> ](https://colab.research.google.com/github/taehojo/deeplearning/blob/master/colab/ch14-colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터의 확인과 검증셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 데이터를 미리 보겠습니다.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                390       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 2s 28ms/step - loss: 7.7530 - accuracy: 0.2510 - val_loss: 3.8738 - val_accuracy: 0.2400\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.7604 - accuracy: 0.4439 - val_loss: 0.3569 - val_accuracy: 0.8162\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3742 - accuracy: 0.7837 - val_loss: 0.3562 - val_accuracy: 0.8054\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3802 - accuracy: 0.8219 - val_loss: 0.3136 - val_accuracy: 0.8792\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3212 - accuracy: 0.8945 - val_loss: 0.2908 - val_accuracy: 0.9300\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3114 - accuracy: 0.9092 - val_loss: 0.2706 - val_accuracy: 0.9338\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2891 - accuracy: 0.9138 - val_loss: 0.2573 - val_accuracy: 0.9338\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2779 - accuracy: 0.9187 - val_loss: 0.2514 - val_accuracy: 0.9408\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2679 - accuracy: 0.9233 - val_loss: 0.2419 - val_accuracy: 0.9431\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2600 - accuracy: 0.9256 - val_loss: 0.2345 - val_accuracy: 0.9454\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2525 - accuracy: 0.9246 - val_loss: 0.2285 - val_accuracy: 0.9446\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2455 - accuracy: 0.9261 - val_loss: 0.2221 - val_accuracy: 0.9423\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2383 - accuracy: 0.9276 - val_loss: 0.2169 - val_accuracy: 0.9415\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2311 - accuracy: 0.9302 - val_loss: 0.2114 - val_accuracy: 0.9423\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2253 - accuracy: 0.9287 - val_loss: 0.2115 - val_accuracy: 0.9392\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2235 - accuracy: 0.9279 - val_loss: 0.2120 - val_accuracy: 0.9338\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2216 - accuracy: 0.9284 - val_loss: 0.2108 - val_accuracy: 0.9377\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2200 - accuracy: 0.9269 - val_loss: 0.2120 - val_accuracy: 0.9362\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2187 - accuracy: 0.9261 - val_loss: 0.2098 - val_accuracy: 0.9369\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2172 - accuracy: 0.9269 - val_loss: 0.2087 - val_accuracy: 0.9362\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2159 - accuracy: 0.9279 - val_loss: 0.2072 - val_accuracy: 0.9369\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2145 - accuracy: 0.9281 - val_loss: 0.2077 - val_accuracy: 0.9369\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2135 - accuracy: 0.9266 - val_loss: 0.2058 - val_accuracy: 0.9369\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2123 - accuracy: 0.9294 - val_loss: 0.2044 - val_accuracy: 0.9369\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2109 - accuracy: 0.9294 - val_loss: 0.2049 - val_accuracy: 0.9377\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2099 - accuracy: 0.9287 - val_loss: 0.2009 - val_accuracy: 0.9385\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2079 - accuracy: 0.9292 - val_loss: 0.1992 - val_accuracy: 0.9385\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2053 - accuracy: 0.9294 - val_loss: 0.1936 - val_accuracy: 0.9392\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2018 - accuracy: 0.9297 - val_loss: 0.1912 - val_accuracy: 0.9408\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2005 - accuracy: 0.9297 - val_loss: 0.1886 - val_accuracy: 0.9446\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1988 - accuracy: 0.9299 - val_loss: 0.1890 - val_accuracy: 0.9415\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1978 - accuracy: 0.9317 - val_loss: 0.1871 - val_accuracy: 0.9454\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1969 - accuracy: 0.9310 - val_loss: 0.1862 - val_accuracy: 0.9438\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1959 - accuracy: 0.9307 - val_loss: 0.1852 - val_accuracy: 0.9438\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1954 - accuracy: 0.9312 - val_loss: 0.1837 - val_accuracy: 0.9446\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1938 - accuracy: 0.9310 - val_loss: 0.1846 - val_accuracy: 0.9400\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1933 - accuracy: 0.9315 - val_loss: 0.1828 - val_accuracy: 0.9438\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1927 - accuracy: 0.9333 - val_loss: 0.1821 - val_accuracy: 0.9446\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1917 - accuracy: 0.9338 - val_loss: 0.1813 - val_accuracy: 0.9454\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1913 - accuracy: 0.9330 - val_loss: 0.1807 - val_accuracy: 0.9454\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1905 - accuracy: 0.9341 - val_loss: 0.1807 - val_accuracy: 0.9423\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1898 - accuracy: 0.9333 - val_loss: 0.1790 - val_accuracy: 0.9438\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1896 - accuracy: 0.9338 - val_loss: 0.1784 - val_accuracy: 0.9438\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1884 - accuracy: 0.9325 - val_loss: 0.1788 - val_accuracy: 0.9415\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1878 - accuracy: 0.9338 - val_loss: 0.1771 - val_accuracy: 0.9446\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1873 - accuracy: 0.9346 - val_loss: 0.1768 - val_accuracy: 0.9431\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1866 - accuracy: 0.9353 - val_loss: 0.1759 - val_accuracy: 0.9438\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1859 - accuracy: 0.9356 - val_loss: 0.1757 - val_accuracy: 0.9423\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1854 - accuracy: 0.9341 - val_loss: 0.1747 - val_accuracy: 0.9438\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1852 - accuracy: 0.9353 - val_loss: 0.1737 - val_accuracy: 0.9446\n"
     ]
    }
   ],
   "source": [
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25) # 0.8 x 0.25 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 7ms/step - loss: 0.1810 - accuracy: 0.9392\n",
      "Test accuracy: 0.939230740070343\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 업데이트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델의 저장 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./data/model/all\\01-0.7554.hdf5\n",
      "\n",
      "Epoch 2: saving model to ./data/model/all\\02-0.7554.hdf5\n",
      "\n",
      "Epoch 3: saving model to ./data/model/all\\03-0.7554.hdf5\n",
      "\n",
      "Epoch 4: saving model to ./data/model/all\\04-0.7846.hdf5\n",
      "\n",
      "Epoch 5: saving model to ./data/model/all\\05-0.8346.hdf5\n",
      "\n",
      "Epoch 6: saving model to ./data/model/all\\06-0.8546.hdf5\n",
      "\n",
      "Epoch 7: saving model to ./data/model/all\\07-0.8738.hdf5\n",
      "\n",
      "Epoch 8: saving model to ./data/model/all\\08-0.8838.hdf5\n",
      "\n",
      "Epoch 9: saving model to ./data/model/all\\09-0.8862.hdf5\n",
      "\n",
      "Epoch 10: saving model to ./data/model/all\\10-0.9008.hdf5\n",
      "\n",
      "Epoch 11: saving model to ./data/model/all\\11-0.9008.hdf5\n",
      "\n",
      "Epoch 12: saving model to ./data/model/all\\12-0.9054.hdf5\n",
      "\n",
      "Epoch 13: saving model to ./data/model/all\\13-0.9115.hdf5\n",
      "\n",
      "Epoch 14: saving model to ./data/model/all\\14-0.9200.hdf5\n",
      "\n",
      "Epoch 15: saving model to ./data/model/all\\15-0.9269.hdf5\n",
      "\n",
      "Epoch 16: saving model to ./data/model/all\\16-0.9292.hdf5\n",
      "\n",
      "Epoch 17: saving model to ./data/model/all\\17-0.9331.hdf5\n",
      "\n",
      "Epoch 18: saving model to ./data/model/all\\18-0.9315.hdf5\n",
      "\n",
      "Epoch 19: saving model to ./data/model/all\\19-0.9354.hdf5\n",
      "\n",
      "Epoch 20: saving model to ./data/model/all\\20-0.9354.hdf5\n",
      "\n",
      "Epoch 21: saving model to ./data/model/all\\21-0.9377.hdf5\n",
      "\n",
      "Epoch 22: saving model to ./data/model/all\\22-0.9400.hdf5\n",
      "\n",
      "Epoch 23: saving model to ./data/model/all\\23-0.9400.hdf5\n",
      "\n",
      "Epoch 24: saving model to ./data/model/all\\24-0.9385.hdf5\n",
      "\n",
      "Epoch 25: saving model to ./data/model/all\\25-0.9408.hdf5\n",
      "\n",
      "Epoch 26: saving model to ./data/model/all\\26-0.9385.hdf5\n",
      "\n",
      "Epoch 27: saving model to ./data/model/all\\27-0.9408.hdf5\n",
      "\n",
      "Epoch 28: saving model to ./data/model/all\\28-0.9400.hdf5\n",
      "\n",
      "Epoch 29: saving model to ./data/model/all\\29-0.9400.hdf5\n",
      "\n",
      "Epoch 30: saving model to ./data/model/all\\30-0.9408.hdf5\n",
      "\n",
      "Epoch 31: saving model to ./data/model/all\\31-0.9400.hdf5\n",
      "\n",
      "Epoch 32: saving model to ./data/model/all\\32-0.9408.hdf5\n",
      "\n",
      "Epoch 33: saving model to ./data/model/all\\33-0.9408.hdf5\n",
      "\n",
      "Epoch 34: saving model to ./data/model/all\\34-0.9431.hdf5\n",
      "\n",
      "Epoch 35: saving model to ./data/model/all\\35-0.9408.hdf5\n",
      "\n",
      "Epoch 36: saving model to ./data/model/all\\36-0.9431.hdf5\n",
      "\n",
      "Epoch 37: saving model to ./data/model/all\\37-0.9408.hdf5\n",
      "\n",
      "Epoch 38: saving model to ./data/model/all\\38-0.9431.hdf5\n",
      "\n",
      "Epoch 39: saving model to ./data/model/all\\39-0.9446.hdf5\n",
      "\n",
      "Epoch 40: saving model to ./data/model/all\\40-0.9415.hdf5\n",
      "\n",
      "Epoch 41: saving model to ./data/model/all\\41-0.9446.hdf5\n",
      "\n",
      "Epoch 42: saving model to ./data/model/all\\42-0.9423.hdf5\n",
      "\n",
      "Epoch 43: saving model to ./data/model/all\\43-0.9446.hdf5\n",
      "\n",
      "Epoch 44: saving model to ./data/model/all\\44-0.9446.hdf5\n",
      "\n",
      "Epoch 45: saving model to ./data/model/all\\45-0.9446.hdf5\n",
      "\n",
      "Epoch 46: saving model to ./data/model/all\\46-0.9469.hdf5\n",
      "\n",
      "Epoch 47: saving model to ./data/model/all\\47-0.9454.hdf5\n",
      "\n",
      "Epoch 48: saving model to ./data/model/all\\48-0.9454.hdf5\n",
      "\n",
      "Epoch 49: saving model to ./data/model/all\\49-0.9469.hdf5\n",
      "\n",
      "Epoch 50: saving model to ./data/model/all\\50-0.9485.hdf5\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장의 조건을 설정합니다.\n",
    "modelpath=\"./data/model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1)\n",
    "\n",
    "# 모델을 실행합니다. \n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25, verbose=0, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 5ms/step - loss: 0.1430 - accuracy: 0.9431\n",
      "Test accuracy: 0.9430769085884094\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 그래프로 과적합 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 확인을 위한 긴 학습 (컴퓨터 환경에 따라 시간이 다소 걸릴수 있습니다)\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, verbose=0, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.140321</td>\n",
       "      <td>0.952271</td>\n",
       "      <td>0.147043</td>\n",
       "      <td>0.948462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139432</td>\n",
       "      <td>0.952784</td>\n",
       "      <td>0.148432</td>\n",
       "      <td>0.946923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.138635</td>\n",
       "      <td>0.953041</td>\n",
       "      <td>0.143959</td>\n",
       "      <td>0.949231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.137957</td>\n",
       "      <td>0.952271</td>\n",
       "      <td>0.143118</td>\n",
       "      <td>0.950769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135013</td>\n",
       "      <td>0.953297</td>\n",
       "      <td>0.140777</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.022088</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.065796</td>\n",
       "      <td>0.987692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.019938</td>\n",
       "      <td>0.996408</td>\n",
       "      <td>0.064169</td>\n",
       "      <td>0.987692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.019631</td>\n",
       "      <td>0.995894</td>\n",
       "      <td>0.060643</td>\n",
       "      <td>0.987692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.020764</td>\n",
       "      <td>0.995894</td>\n",
       "      <td>0.065941</td>\n",
       "      <td>0.986923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.020085</td>\n",
       "      <td>0.995381</td>\n",
       "      <td>0.065479</td>\n",
       "      <td>0.987692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  accuracy  val_loss  val_accuracy\n",
       "0     0.140321  0.952271  0.147043      0.948462\n",
       "1     0.139432  0.952784  0.148432      0.946923\n",
       "2     0.138635  0.953041  0.143959      0.949231\n",
       "3     0.137957  0.952271  0.143118      0.950769\n",
       "4     0.135013  0.953297  0.140777      0.950000\n",
       "...        ...       ...       ...           ...\n",
       "1995  0.022088  0.994355  0.065796      0.987692\n",
       "1996  0.019938  0.996408  0.064169      0.987692\n",
       "1997  0.019631  0.995894  0.060643      0.987692\n",
       "1998  0.020764  0.995894  0.065941      0.986923\n",
       "1999  0.020085  0.995381  0.065479      0.987692\n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history에 저장된 학습 결과를 확인해 보겠습니다. \n",
    "hist_df=pd.DataFrame(history.history)\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_vloss에 테스트셋(여기서는 검증셋)의 오차를 저장합니다.\n",
    "y_vloss=hist_df['val_loss']\n",
    "\n",
    "# y_loss에 학습셋의 오차를 저장합니다.\n",
    "y_loss=hist_df['loss']\n",
    "\n",
    "# x 값을 지정하고 테스트셋(검증셋)의 오차를 빨간색으로, 학습셋의 오차를 파란색으로 표시합니다.\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2, label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습의 자동 중단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습의 자동 중단 및 최적화 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 1s 34ms/step - loss: 0.7145 - accuracy: 0.7557 - val_loss: 0.5842 - val_accuracy: 0.7577\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.5046 - accuracy: 0.7557 - val_loss: 0.4307 - val_accuracy: 0.7577\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.4070 - accuracy: 0.7557 - val_loss: 0.3941 - val_accuracy: 0.7577\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.3912 - accuracy: 0.7557 - val_loss: 0.3901 - val_accuracy: 0.7577\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.3878 - accuracy: 0.7557 - val_loss: 0.3850 - val_accuracy: 0.7577\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.3818 - accuracy: 0.7557 - val_loss: 0.3800 - val_accuracy: 0.7577\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3772 - accuracy: 0.7557 - val_loss: 0.3762 - val_accuracy: 0.7577\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.3726 - accuracy: 0.7557 - val_loss: 0.3714 - val_accuracy: 0.7577\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3670 - accuracy: 0.7557 - val_loss: 0.3653 - val_accuracy: 0.7577\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3608 - accuracy: 0.7557 - val_loss: 0.3591 - val_accuracy: 0.7577\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.3545 - accuracy: 0.7562 - val_loss: 0.3531 - val_accuracy: 0.7608\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3482 - accuracy: 0.7608 - val_loss: 0.3464 - val_accuracy: 0.7715\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3408 - accuracy: 0.7783 - val_loss: 0.3386 - val_accuracy: 0.7954\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.3326 - accuracy: 0.8147 - val_loss: 0.3302 - val_accuracy: 0.8123\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3249 - accuracy: 0.8440 - val_loss: 0.3224 - val_accuracy: 0.8431\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3187 - accuracy: 0.8632 - val_loss: 0.3164 - val_accuracy: 0.8677\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3140 - accuracy: 0.8761 - val_loss: 0.3120 - val_accuracy: 0.8800\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3104 - accuracy: 0.8861 - val_loss: 0.3089 - val_accuracy: 0.8892\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3076 - accuracy: 0.8935 - val_loss: 0.3062 - val_accuracy: 0.9000\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3051 - accuracy: 0.8979 - val_loss: 0.3037 - val_accuracy: 0.9031\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3028 - accuracy: 0.8984 - val_loss: 0.3015 - val_accuracy: 0.9069\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.3004 - accuracy: 0.9020 - val_loss: 0.2991 - val_accuracy: 0.9092\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.2980 - accuracy: 0.9061 - val_loss: 0.2965 - val_accuracy: 0.9115\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2954 - accuracy: 0.9071 - val_loss: 0.2938 - val_accuracy: 0.9131\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2930 - accuracy: 0.9110 - val_loss: 0.2912 - val_accuracy: 0.9177\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.2902 - accuracy: 0.9140 - val_loss: 0.2884 - val_accuracy: 0.9185\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2876 - accuracy: 0.9153 - val_loss: 0.2857 - val_accuracy: 0.9208\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2850 - accuracy: 0.9158 - val_loss: 0.2829 - val_accuracy: 0.9277\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2821 - accuracy: 0.9176 - val_loss: 0.2800 - val_accuracy: 0.9277\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2793 - accuracy: 0.9181 - val_loss: 0.2770 - val_accuracy: 0.9292\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2763 - accuracy: 0.9199 - val_loss: 0.2741 - val_accuracy: 0.9315\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2736 - accuracy: 0.9215 - val_loss: 0.2711 - val_accuracy: 0.9331\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2708 - accuracy: 0.9243 - val_loss: 0.2681 - val_accuracy: 0.9346\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2679 - accuracy: 0.9253 - val_loss: 0.2650 - val_accuracy: 0.9346\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2651 - accuracy: 0.9256 - val_loss: 0.2617 - val_accuracy: 0.9362\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2614 - accuracy: 0.9261 - val_loss: 0.2576 - val_accuracy: 0.9362\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2566 - accuracy: 0.9264 - val_loss: 0.2511 - val_accuracy: 0.9362\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2473 - accuracy: 0.9310 - val_loss: 0.2364 - val_accuracy: 0.9400\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2265 - accuracy: 0.9328 - val_loss: 0.2045 - val_accuracy: 0.9415\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2074 - accuracy: 0.9312 - val_loss: 0.1883 - val_accuracy: 0.9369\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1968 - accuracy: 0.9341 - val_loss: 0.1800 - val_accuracy: 0.9400\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.1920 - accuracy: 0.9341 - val_loss: 0.1776 - val_accuracy: 0.9400\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1913 - accuracy: 0.9353 - val_loss: 0.1760 - val_accuracy: 0.9415\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1893 - accuracy: 0.9358 - val_loss: 0.1733 - val_accuracy: 0.9400\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1886 - accuracy: 0.9358 - val_loss: 0.1721 - val_accuracy: 0.9400\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.1870 - accuracy: 0.9371 - val_loss: 0.1716 - val_accuracy: 0.9415\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1859 - accuracy: 0.9371 - val_loss: 0.1713 - val_accuracy: 0.9431\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1844 - accuracy: 0.9382 - val_loss: 0.1710 - val_accuracy: 0.9438\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1832 - accuracy: 0.9387 - val_loss: 0.1690 - val_accuracy: 0.9454\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1816 - accuracy: 0.9392 - val_loss: 0.1690 - val_accuracy: 0.9446\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1795 - accuracy: 0.9387 - val_loss: 0.1640 - val_accuracy: 0.9446\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1783 - accuracy: 0.9397 - val_loss: 0.1620 - val_accuracy: 0.9454\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1764 - accuracy: 0.9402 - val_loss: 0.1606 - val_accuracy: 0.9454\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1720 - accuracy: 0.9412 - val_loss: 0.1585 - val_accuracy: 0.9469\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1732 - accuracy: 0.9430 - val_loss: 0.1628 - val_accuracy: 0.9392\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1727 - accuracy: 0.9405 - val_loss: 0.1581 - val_accuracy: 0.9446\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1693 - accuracy: 0.9397 - val_loss: 0.1525 - val_accuracy: 0.9462\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1670 - accuracy: 0.9425 - val_loss: 0.1533 - val_accuracy: 0.9477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1632 - accuracy: 0.9461 - val_loss: 0.1496 - val_accuracy: 0.9477\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1613 - accuracy: 0.9466 - val_loss: 0.1466 - val_accuracy: 0.9485\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1580 - accuracy: 0.9464 - val_loss: 0.1493 - val_accuracy: 0.9469\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1571 - accuracy: 0.9453 - val_loss: 0.1430 - val_accuracy: 0.9492\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1512 - accuracy: 0.9484 - val_loss: 0.1380 - val_accuracy: 0.9500\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1470 - accuracy: 0.9502 - val_loss: 0.1344 - val_accuracy: 0.9508\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1459 - accuracy: 0.9505 - val_loss: 0.1429 - val_accuracy: 0.9492\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1447 - accuracy: 0.9494 - val_loss: 0.1370 - val_accuracy: 0.9500\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.1358 - accuracy: 0.9502 - val_loss: 0.1168 - val_accuracy: 0.9523\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1275 - accuracy: 0.9523 - val_loss: 0.1099 - val_accuracy: 0.9585\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1243 - accuracy: 0.9551 - val_loss: 0.1051 - val_accuracy: 0.9600\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1258 - accuracy: 0.9569 - val_loss: 0.1002 - val_accuracy: 0.9592\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1202 - accuracy: 0.9556 - val_loss: 0.1001 - val_accuracy: 0.9531\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1203 - accuracy: 0.9589 - val_loss: 0.1017 - val_accuracy: 0.9546\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1110 - accuracy: 0.9618 - val_loss: 0.0980 - val_accuracy: 0.9700\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.1112 - accuracy: 0.9610 - val_loss: 0.0906 - val_accuracy: 0.9615\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1113 - accuracy: 0.9620 - val_loss: 0.1006 - val_accuracy: 0.9546\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1121 - accuracy: 0.9613 - val_loss: 0.0932 - val_accuracy: 0.9585\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1099 - accuracy: 0.9607 - val_loss: 0.0929 - val_accuracy: 0.9577\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1060 - accuracy: 0.9638 - val_loss: 0.0828 - val_accuracy: 0.9685\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1044 - accuracy: 0.9664 - val_loss: 0.0819 - val_accuracy: 0.9708\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1004 - accuracy: 0.9700 - val_loss: 0.0894 - val_accuracy: 0.9700\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1029 - accuracy: 0.9633 - val_loss: 0.0875 - val_accuracy: 0.9700\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0972 - accuracy: 0.9687 - val_loss: 0.0771 - val_accuracy: 0.9738\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0953 - accuracy: 0.9713 - val_loss: 0.0763 - val_accuracy: 0.9746\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0933 - accuracy: 0.9725 - val_loss: 0.0771 - val_accuracy: 0.9746\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0932 - accuracy: 0.9715 - val_loss: 0.0742 - val_accuracy: 0.9754\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0946 - accuracy: 0.9713 - val_loss: 0.0768 - val_accuracy: 0.9715\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0931 - accuracy: 0.9715 - val_loss: 0.0708 - val_accuracy: 0.9769\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0895 - accuracy: 0.9725 - val_loss: 0.0692 - val_accuracy: 0.9769\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0884 - accuracy: 0.9738 - val_loss: 0.0735 - val_accuracy: 0.9731\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0902 - accuracy: 0.9710 - val_loss: 0.0693 - val_accuracy: 0.9762\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0910 - accuracy: 0.9715 - val_loss: 0.0665 - val_accuracy: 0.9769\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0890 - accuracy: 0.9720 - val_loss: 0.0662 - val_accuracy: 0.9777\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0893 - accuracy: 0.9718 - val_loss: 0.0663 - val_accuracy: 0.9792\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0861 - accuracy: 0.9743 - val_loss: 0.0640 - val_accuracy: 0.9800\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0833 - accuracy: 0.9749 - val_loss: 0.0653 - val_accuracy: 0.9800\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0828 - accuracy: 0.9754 - val_loss: 0.0629 - val_accuracy: 0.9792\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0818 - accuracy: 0.9751 - val_loss: 0.0684 - val_accuracy: 0.9808\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0837 - accuracy: 0.9766 - val_loss: 0.0669 - val_accuracy: 0.9815\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0836 - accuracy: 0.9761 - val_loss: 0.0619 - val_accuracy: 0.9823\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0804 - accuracy: 0.9751 - val_loss: 0.0630 - val_accuracy: 0.9815\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0825 - accuracy: 0.9764 - val_loss: 0.0665 - val_accuracy: 0.9823\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0860 - accuracy: 0.9743 - val_loss: 0.0783 - val_accuracy: 0.9777\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0898 - accuracy: 0.9738 - val_loss: 0.0860 - val_accuracy: 0.9738\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0914 - accuracy: 0.9697 - val_loss: 0.0753 - val_accuracy: 0.9792\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0858 - accuracy: 0.9728 - val_loss: 0.0582 - val_accuracy: 0.9808\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0832 - accuracy: 0.9743 - val_loss: 0.0578 - val_accuracy: 0.9823\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0796 - accuracy: 0.9774 - val_loss: 0.0602 - val_accuracy: 0.9831\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0816 - accuracy: 0.9749 - val_loss: 0.0659 - val_accuracy: 0.9815\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0774 - accuracy: 0.9759 - val_loss: 0.0552 - val_accuracy: 0.9823\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0768 - accuracy: 0.9764 - val_loss: 0.0552 - val_accuracy: 0.9831\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0761 - accuracy: 0.9782 - val_loss: 0.0566 - val_accuracy: 0.9792\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0748 - accuracy: 0.9769 - val_loss: 0.0535 - val_accuracy: 0.9823\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0741 - accuracy: 0.9777 - val_loss: 0.0540 - val_accuracy: 0.9815\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0734 - accuracy: 0.9774 - val_loss: 0.0539 - val_accuracy: 0.9862\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0733 - accuracy: 0.9784 - val_loss: 0.0526 - val_accuracy: 0.9846\n",
      "Epoch 116/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0732 - accuracy: 0.9777 - val_loss: 0.0532 - val_accuracy: 0.9815\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0765 - accuracy: 0.9766 - val_loss: 0.0516 - val_accuracy: 0.9854\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0729 - accuracy: 0.9769 - val_loss: 0.0572 - val_accuracy: 0.9792\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0747 - accuracy: 0.9769 - val_loss: 0.0515 - val_accuracy: 0.9823\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0715 - accuracy: 0.9792 - val_loss: 0.0508 - val_accuracy: 0.9846\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0714 - accuracy: 0.9792 - val_loss: 0.0512 - val_accuracy: 0.9862\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0731 - accuracy: 0.9774 - val_loss: 0.0516 - val_accuracy: 0.9869\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0733 - accuracy: 0.9792 - val_loss: 0.0513 - val_accuracy: 0.9869\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0725 - accuracy: 0.9784 - val_loss: 0.0525 - val_accuracy: 0.9869\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0709 - accuracy: 0.9777 - val_loss: 0.0513 - val_accuracy: 0.9862\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0699 - accuracy: 0.9795 - val_loss: 0.0535 - val_accuracy: 0.9862\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0724 - accuracy: 0.9779 - val_loss: 0.0592 - val_accuracy: 0.9862\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0740 - accuracy: 0.9756 - val_loss: 0.0532 - val_accuracy: 0.9862\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0697 - accuracy: 0.9787 - val_loss: 0.0483 - val_accuracy: 0.9877\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0699 - accuracy: 0.9790 - val_loss: 0.0469 - val_accuracy: 0.9869\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0682 - accuracy: 0.9790 - val_loss: 0.0469 - val_accuracy: 0.9862\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0673 - accuracy: 0.9779 - val_loss: 0.0464 - val_accuracy: 0.9854\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0662 - accuracy: 0.9790 - val_loss: 0.0459 - val_accuracy: 0.9869\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0664 - accuracy: 0.9800 - val_loss: 0.0458 - val_accuracy: 0.9862\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0661 - accuracy: 0.9808 - val_loss: 0.0451 - val_accuracy: 0.9854\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0680 - accuracy: 0.9792 - val_loss: 0.0449 - val_accuracy: 0.9877\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0683 - accuracy: 0.9774 - val_loss: 0.0444 - val_accuracy: 0.9854\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0646 - accuracy: 0.9800 - val_loss: 0.0454 - val_accuracy: 0.9869\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0648 - accuracy: 0.9795 - val_loss: 0.0445 - val_accuracy: 0.9862\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0649 - accuracy: 0.9808 - val_loss: 0.0458 - val_accuracy: 0.9869\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0660 - accuracy: 0.9792 - val_loss: 0.0483 - val_accuracy: 0.9877\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0652 - accuracy: 0.9795 - val_loss: 0.0436 - val_accuracy: 0.9869\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0649 - accuracy: 0.9797 - val_loss: 0.0443 - val_accuracy: 0.9869\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0638 - accuracy: 0.9790 - val_loss: 0.0445 - val_accuracy: 0.9869\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0657 - accuracy: 0.9779 - val_loss: 0.0423 - val_accuracy: 0.9869\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0650 - accuracy: 0.9779 - val_loss: 0.0421 - val_accuracy: 0.9869\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0625 - accuracy: 0.9795 - val_loss: 0.0420 - val_accuracy: 0.9862\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0628 - accuracy: 0.9808 - val_loss: 0.0444 - val_accuracy: 0.9831\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0662 - accuracy: 0.9797 - val_loss: 0.0430 - val_accuracy: 0.9831\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0682 - accuracy: 0.9782 - val_loss: 0.0452 - val_accuracy: 0.9831\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0672 - accuracy: 0.9795 - val_loss: 0.0432 - val_accuracy: 0.9831\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0621 - accuracy: 0.9813 - val_loss: 0.0414 - val_accuracy: 0.9869\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0620 - accuracy: 0.9802 - val_loss: 0.0407 - val_accuracy: 0.9846\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0614 - accuracy: 0.9810 - val_loss: 0.0399 - val_accuracy: 0.9885\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0607 - accuracy: 0.9810 - val_loss: 0.0408 - val_accuracy: 0.9838\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0614 - accuracy: 0.9815 - val_loss: 0.0389 - val_accuracy: 0.9869\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0605 - accuracy: 0.9800 - val_loss: 0.0406 - val_accuracy: 0.9846\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0605 - accuracy: 0.9805 - val_loss: 0.0388 - val_accuracy: 0.9877\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0608 - accuracy: 0.9805 - val_loss: 0.0387 - val_accuracy: 0.9869\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0609 - accuracy: 0.9800 - val_loss: 0.0383 - val_accuracy: 0.9869\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0618 - accuracy: 0.9810 - val_loss: 0.0381 - val_accuracy: 0.9885\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0622 - accuracy: 0.9802 - val_loss: 0.0441 - val_accuracy: 0.9892\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0636 - accuracy: 0.9797 - val_loss: 0.0381 - val_accuracy: 0.9892\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0592 - accuracy: 0.9820 - val_loss: 0.0381 - val_accuracy: 0.9877\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0590 - accuracy: 0.9815 - val_loss: 0.0380 - val_accuracy: 0.9885\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0628 - accuracy: 0.9800 - val_loss: 0.0452 - val_accuracy: 0.9877\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0607 - accuracy: 0.9815 - val_loss: 0.0394 - val_accuracy: 0.9892\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0614 - accuracy: 0.9805 - val_loss: 0.0393 - val_accuracy: 0.9900\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0622 - accuracy: 0.9797 - val_loss: 0.0442 - val_accuracy: 0.9877\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0636 - accuracy: 0.9795 - val_loss: 0.0371 - val_accuracy: 0.9885\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0578 - accuracy: 0.9810 - val_loss: 0.0359 - val_accuracy: 0.9892\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0571 - accuracy: 0.9818 - val_loss: 0.0358 - val_accuracy: 0.9892\n",
      "Epoch 173/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0569 - accuracy: 0.9818 - val_loss: 0.0358 - val_accuracy: 0.9885\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0571 - accuracy: 0.9823 - val_loss: 0.0352 - val_accuracy: 0.9885\n",
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0585 - accuracy: 0.9823 - val_loss: 0.0347 - val_accuracy: 0.9892\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0570 - accuracy: 0.9836 - val_loss: 0.0352 - val_accuracy: 0.9892\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0568 - accuracy: 0.9826 - val_loss: 0.0372 - val_accuracy: 0.9846\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0582 - accuracy: 0.9820 - val_loss: 0.0362 - val_accuracy: 0.9877\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0572 - accuracy: 0.9823 - val_loss: 0.0351 - val_accuracy: 0.9885\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0567 - accuracy: 0.9820 - val_loss: 0.0357 - val_accuracy: 0.9854\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0561 - accuracy: 0.9826 - val_loss: 0.0341 - val_accuracy: 0.9908\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0564 - accuracy: 0.9831 - val_loss: 0.0339 - val_accuracy: 0.9892\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0551 - accuracy: 0.9820 - val_loss: 0.0337 - val_accuracy: 0.9892\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0548 - accuracy: 0.9836 - val_loss: 0.0335 - val_accuracy: 0.9908\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0555 - accuracy: 0.9828 - val_loss: 0.0333 - val_accuracy: 0.9892\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0568 - accuracy: 0.9823 - val_loss: 0.0353 - val_accuracy: 0.9869\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0585 - accuracy: 0.9808 - val_loss: 0.0341 - val_accuracy: 0.9915\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0561 - accuracy: 0.9826 - val_loss: 0.0363 - val_accuracy: 0.9923\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0561 - accuracy: 0.9826 - val_loss: 0.0333 - val_accuracy: 0.9900\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0549 - accuracy: 0.9831 - val_loss: 0.0328 - val_accuracy: 0.9908\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0549 - accuracy: 0.9831 - val_loss: 0.0323 - val_accuracy: 0.9908\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0338 - val_accuracy: 0.9869\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0557 - accuracy: 0.9836 - val_loss: 0.0333 - val_accuracy: 0.9892\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0536 - accuracy: 0.9846 - val_loss: 0.0319 - val_accuracy: 0.9908\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0551 - accuracy: 0.9826 - val_loss: 0.0322 - val_accuracy: 0.9915\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0553 - accuracy: 0.9828 - val_loss: 0.0365 - val_accuracy: 0.9838\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0580 - accuracy: 0.9818 - val_loss: 0.0373 - val_accuracy: 0.9838\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0546 - accuracy: 0.9843 - val_loss: 0.0331 - val_accuracy: 0.9877\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0545 - accuracy: 0.9841 - val_loss: 0.0377 - val_accuracy: 0.9838\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0549 - accuracy: 0.9841 - val_loss: 0.0318 - val_accuracy: 0.9900\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0535 - accuracy: 0.9838 - val_loss: 0.0349 - val_accuracy: 0.9838\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0566 - accuracy: 0.9823 - val_loss: 0.0370 - val_accuracy: 0.9838\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0618 - accuracy: 0.9797 - val_loss: 0.0407 - val_accuracy: 0.9831\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0569 - accuracy: 0.9820 - val_loss: 0.0329 - val_accuracy: 0.9869\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0579 - accuracy: 0.9820 - val_loss: 0.0376 - val_accuracy: 0.9838\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0613 - accuracy: 0.9818 - val_loss: 0.0412 - val_accuracy: 0.9815\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0589 - accuracy: 0.9820 - val_loss: 0.0313 - val_accuracy: 0.9915\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0317 - val_accuracy: 0.9885\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0304 - val_accuracy: 0.9908\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0514 - accuracy: 0.9854 - val_loss: 0.0324 - val_accuracy: 0.9877\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0575 - accuracy: 0.9805 - val_loss: 0.0364 - val_accuracy: 0.9838\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0540 - accuracy: 0.9836 - val_loss: 0.0341 - val_accuracy: 0.9854\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0545 - accuracy: 0.9831 - val_loss: 0.0457 - val_accuracy: 0.9792\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0574 - accuracy: 0.9823 - val_loss: 0.0479 - val_accuracy: 0.9792\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0602 - accuracy: 0.9808 - val_loss: 0.0340 - val_accuracy: 0.9846\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0542 - accuracy: 0.9836 - val_loss: 0.0317 - val_accuracy: 0.9877\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0534 - accuracy: 0.9831 - val_loss: 0.0311 - val_accuracy: 0.9900\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0568 - accuracy: 0.9838 - val_loss: 0.0308 - val_accuracy: 0.9892\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0532 - accuracy: 0.9838 - val_loss: 0.0298 - val_accuracy: 0.9938\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0517 - accuracy: 0.9843 - val_loss: 0.0295 - val_accuracy: 0.9931\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0516 - accuracy: 0.9851 - val_loss: 0.0294 - val_accuracy: 0.9938\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0515 - accuracy: 0.9849 - val_loss: 0.0290 - val_accuracy: 0.9931\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0504 - accuracy: 0.9859 - val_loss: 0.0308 - val_accuracy: 0.9885\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0524 - accuracy: 0.9838 - val_loss: 0.0292 - val_accuracy: 0.9931\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0511 - accuracy: 0.9851 - val_loss: 0.0294 - val_accuracy: 0.9938\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0505 - accuracy: 0.9859 - val_loss: 0.0288 - val_accuracy: 0.9931\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0512 - accuracy: 0.9846 - val_loss: 0.0293 - val_accuracy: 0.9946\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0509 - accuracy: 0.9859 - val_loss: 0.0305 - val_accuracy: 0.9931\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0499 - accuracy: 0.9856 - val_loss: 0.0285 - val_accuracy: 0.9931\n",
      "Epoch 230/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0499 - accuracy: 0.9856 - val_loss: 0.0298 - val_accuracy: 0.9938\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0531 - accuracy: 0.9843 - val_loss: 0.0321 - val_accuracy: 0.9931\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0546 - accuracy: 0.9831 - val_loss: 0.0312 - val_accuracy: 0.9923\n",
      "Epoch 233/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0510 - accuracy: 0.9856 - val_loss: 0.0290 - val_accuracy: 0.9938\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0511 - accuracy: 0.9854 - val_loss: 0.0283 - val_accuracy: 0.9938\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0509 - accuracy: 0.9849 - val_loss: 0.0285 - val_accuracy: 0.9938\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0495 - accuracy: 0.9859 - val_loss: 0.0283 - val_accuracy: 0.9938\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0493 - accuracy: 0.9867 - val_loss: 0.0280 - val_accuracy: 0.9938\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0489 - accuracy: 0.9864 - val_loss: 0.0291 - val_accuracy: 0.9938\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0493 - accuracy: 0.9856 - val_loss: 0.0297 - val_accuracy: 0.9885\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0505 - accuracy: 0.9851 - val_loss: 0.0296 - val_accuracy: 0.9908\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0493 - accuracy: 0.9861 - val_loss: 0.0286 - val_accuracy: 0.9931\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0549 - accuracy: 0.9820 - val_loss: 0.0290 - val_accuracy: 0.9915\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0505 - accuracy: 0.9849 - val_loss: 0.0275 - val_accuracy: 0.9938\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0520 - accuracy: 0.9843 - val_loss: 0.0278 - val_accuracy: 0.9938\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0502 - accuracy: 0.9846 - val_loss: 0.0289 - val_accuracy: 0.9938\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0511 - accuracy: 0.9851 - val_loss: 0.0327 - val_accuracy: 0.9915\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0503 - accuracy: 0.9856 - val_loss: 0.0304 - val_accuracy: 0.9931\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0491 - accuracy: 0.9856 - val_loss: 0.0327 - val_accuracy: 0.9915\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0515 - accuracy: 0.9843 - val_loss: 0.0307 - val_accuracy: 0.9915\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0491 - accuracy: 0.9859 - val_loss: 0.0271 - val_accuracy: 0.9946\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0488 - accuracy: 0.9854 - val_loss: 0.0270 - val_accuracy: 0.9931\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0478 - accuracy: 0.9856 - val_loss: 0.0288 - val_accuracy: 0.9938\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0483 - accuracy: 0.9869 - val_loss: 0.0287 - val_accuracy: 0.9900\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0490 - accuracy: 0.9864 - val_loss: 0.0271 - val_accuracy: 0.9931\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0500 - accuracy: 0.9856 - val_loss: 0.0266 - val_accuracy: 0.9931\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0527 - accuracy: 0.9851 - val_loss: 0.0267 - val_accuracy: 0.9938\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0542 - accuracy: 0.9841 - val_loss: 0.0328 - val_accuracy: 0.9838\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0495 - accuracy: 0.9851 - val_loss: 0.0291 - val_accuracy: 0.9892\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0508 - accuracy: 0.9843 - val_loss: 0.0287 - val_accuracy: 0.9892\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0477 - accuracy: 0.9859 - val_loss: 0.0263 - val_accuracy: 0.9938\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0472 - accuracy: 0.9867 - val_loss: 0.0265 - val_accuracy: 0.9938\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0476 - accuracy: 0.9867 - val_loss: 0.0261 - val_accuracy: 0.9938\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0488 - accuracy: 0.9856 - val_loss: 0.0265 - val_accuracy: 0.9931\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0475 - accuracy: 0.9872 - val_loss: 0.0260 - val_accuracy: 0.9938\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0478 - accuracy: 0.9864 - val_loss: 0.0292 - val_accuracy: 0.9938\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0480 - accuracy: 0.9861 - val_loss: 0.0259 - val_accuracy: 0.9938\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0479 - accuracy: 0.9859 - val_loss: 0.0255 - val_accuracy: 0.9938\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0471 - accuracy: 0.9867 - val_loss: 0.0258 - val_accuracy: 0.9938\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0477 - accuracy: 0.9867 - val_loss: 0.0274 - val_accuracy: 0.9938\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0470 - accuracy: 0.9867 - val_loss: 0.0260 - val_accuracy: 0.9938\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0464 - accuracy: 0.9874 - val_loss: 0.0254 - val_accuracy: 0.9938\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0474 - accuracy: 0.9872 - val_loss: 0.0257 - val_accuracy: 0.9938\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0476 - accuracy: 0.9867 - val_loss: 0.0254 - val_accuracy: 0.9938\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0470 - accuracy: 0.9864 - val_loss: 0.0259 - val_accuracy: 0.9915\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0480 - accuracy: 0.9861 - val_loss: 0.0254 - val_accuracy: 0.9931\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0492 - accuracy: 0.9856 - val_loss: 0.0284 - val_accuracy: 0.9892\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0471 - accuracy: 0.9872 - val_loss: 0.0260 - val_accuracy: 0.9923\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0471 - accuracy: 0.9869 - val_loss: 0.0286 - val_accuracy: 0.9885\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0478 - accuracy: 0.9864 - val_loss: 0.0260 - val_accuracy: 0.9923\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0491 - accuracy: 0.9859 - val_loss: 0.0262 - val_accuracy: 0.9915\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0511 - accuracy: 0.9864 - val_loss: 0.0271 - val_accuracy: 0.9915\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0504 - accuracy: 0.9851 - val_loss: 0.0256 - val_accuracy: 0.9946\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0481 - accuracy: 0.9861 - val_loss: 0.0247 - val_accuracy: 0.9938\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0472 - accuracy: 0.9861 - val_loss: 0.0248 - val_accuracy: 0.9946\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0473 - accuracy: 0.9877 - val_loss: 0.0266 - val_accuracy: 0.9915\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0494 - accuracy: 0.9861 - val_loss: 0.0269 - val_accuracy: 0.9908\n",
      "Epoch 287/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0455 - accuracy: 0.9879 - val_loss: 0.0261 - val_accuracy: 0.9915\n",
      "Epoch 288/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0470 - accuracy: 0.9874 - val_loss: 0.0241 - val_accuracy: 0.9946\n",
      "Epoch 289/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0470 - accuracy: 0.9869 - val_loss: 0.0256 - val_accuracy: 0.9915\n",
      "Epoch 290/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0478 - accuracy: 0.9874 - val_loss: 0.0259 - val_accuracy: 0.9915\n",
      "Epoch 291/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0459 - accuracy: 0.9874 - val_loss: 0.0255 - val_accuracy: 0.9915\n",
      "Epoch 292/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0517 - accuracy: 0.9846 - val_loss: 0.0283 - val_accuracy: 0.9892\n",
      "Epoch 293/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0486 - accuracy: 0.9861 - val_loss: 0.0250 - val_accuracy: 0.9931\n",
      "Epoch 294/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0491 - accuracy: 0.9864 - val_loss: 0.0276 - val_accuracy: 0.9900\n",
      "Epoch 295/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0496 - accuracy: 0.9859 - val_loss: 0.0282 - val_accuracy: 0.9900\n",
      "Epoch 296/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0458 - accuracy: 0.9874 - val_loss: 0.0241 - val_accuracy: 0.9938\n",
      "Epoch 297/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0474 - accuracy: 0.9877 - val_loss: 0.0236 - val_accuracy: 0.9946\n",
      "Epoch 298/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0459 - accuracy: 0.9869 - val_loss: 0.0241 - val_accuracy: 0.9946\n",
      "Epoch 299/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0460 - accuracy: 0.9872 - val_loss: 0.0247 - val_accuracy: 0.9915\n",
      "Epoch 300/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0459 - accuracy: 0.9856 - val_loss: 0.0241 - val_accuracy: 0.9938\n",
      "Epoch 301/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0471 - accuracy: 0.9882 - val_loss: 0.0271 - val_accuracy: 0.9931\n",
      "Epoch 302/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0485 - accuracy: 0.9861 - val_loss: 0.0277 - val_accuracy: 0.9915\n",
      "Epoch 303/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0493 - accuracy: 0.9836 - val_loss: 0.0240 - val_accuracy: 0.9938\n",
      "Epoch 304/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0476 - accuracy: 0.9861 - val_loss: 0.0234 - val_accuracy: 0.9938\n",
      "Epoch 305/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0469 - accuracy: 0.9882 - val_loss: 0.0241 - val_accuracy: 0.9938\n",
      "Epoch 306/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0465 - accuracy: 0.9861 - val_loss: 0.0244 - val_accuracy: 0.9938\n",
      "Epoch 307/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0456 - accuracy: 0.9874 - val_loss: 0.0248 - val_accuracy: 0.9938\n",
      "Epoch 308/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0454 - accuracy: 0.9874 - val_loss: 0.0255 - val_accuracy: 0.9931\n",
      "Epoch 309/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0444 - accuracy: 0.9877 - val_loss: 0.0250 - val_accuracy: 0.9915\n",
      "Epoch 310/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0455 - accuracy: 0.9877 - val_loss: 0.0250 - val_accuracy: 0.9915\n",
      "Epoch 311/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0471 - accuracy: 0.9877 - val_loss: 0.0238 - val_accuracy: 0.9931\n",
      "Epoch 312/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0442 - accuracy: 0.9879 - val_loss: 0.0253 - val_accuracy: 0.9931\n",
      "Epoch 313/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0449 - accuracy: 0.9877 - val_loss: 0.0236 - val_accuracy: 0.9938\n",
      "Epoch 314/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0452 - accuracy: 0.9877 - val_loss: 0.0284 - val_accuracy: 0.9900\n",
      "Epoch 315/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0485 - accuracy: 0.9872 - val_loss: 0.0255 - val_accuracy: 0.9938\n",
      "Epoch 316/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0473 - accuracy: 0.9869 - val_loss: 0.0321 - val_accuracy: 0.9892\n",
      "Epoch 317/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0472 - accuracy: 0.9874 - val_loss: 0.0280 - val_accuracy: 0.9908\n",
      "Epoch 318/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0449 - accuracy: 0.9879 - val_loss: 0.0232 - val_accuracy: 0.9938\n",
      "Epoch 319/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0442 - accuracy: 0.9885 - val_loss: 0.0226 - val_accuracy: 0.9946\n",
      "Epoch 320/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0442 - accuracy: 0.9877 - val_loss: 0.0234 - val_accuracy: 0.9938\n",
      "Epoch 321/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.0233 - val_accuracy: 0.9938\n",
      "Epoch 322/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0458 - accuracy: 0.9885 - val_loss: 0.0233 - val_accuracy: 0.9931\n",
      "Epoch 323/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0463 - accuracy: 0.9877 - val_loss: 0.0293 - val_accuracy: 0.9877\n",
      "Epoch 324/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0505 - accuracy: 0.9859 - val_loss: 0.0254 - val_accuracy: 0.9915\n",
      "Epoch 325/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0467 - accuracy: 0.9861 - val_loss: 0.0226 - val_accuracy: 0.9946\n",
      "Epoch 326/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0443 - accuracy: 0.9885 - val_loss: 0.0237 - val_accuracy: 0.9931\n",
      "Epoch 327/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0441 - accuracy: 0.9882 - val_loss: 0.0261 - val_accuracy: 0.9908\n",
      "Epoch 328/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0443 - accuracy: 0.9885 - val_loss: 0.0234 - val_accuracy: 0.9938\n",
      "Epoch 329/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0444 - accuracy: 0.9885 - val_loss: 0.0252 - val_accuracy: 0.9938\n",
      "Epoch 330/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0458 - accuracy: 0.9861 - val_loss: 0.0226 - val_accuracy: 0.9938\n",
      "Epoch 331/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0449 - accuracy: 0.9872 - val_loss: 0.0223 - val_accuracy: 0.9938\n",
      "Epoch 332/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0454 - accuracy: 0.9877 - val_loss: 0.0244 - val_accuracy: 0.9908\n",
      "Epoch 333/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0472 - accuracy: 0.9882 - val_loss: 0.0243 - val_accuracy: 0.9923\n",
      "Epoch 334/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0442 - accuracy: 0.9887 - val_loss: 0.0243 - val_accuracy: 0.9923\n",
      "Epoch 335/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0457 - accuracy: 0.9874 - val_loss: 0.0240 - val_accuracy: 0.9938\n",
      "Epoch 336/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0455 - accuracy: 0.9879 - val_loss: 0.0296 - val_accuracy: 0.9900\n",
      "Epoch 337/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0502 - accuracy: 0.9859 - val_loss: 0.0226 - val_accuracy: 0.9938\n",
      "Epoch 338/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0447 - accuracy: 0.9867 - val_loss: 0.0221 - val_accuracy: 0.9946\n",
      "Epoch 339/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0445 - accuracy: 0.9882 - val_loss: 0.0225 - val_accuracy: 0.9946\n",
      "Epoch 340/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0435 - accuracy: 0.9882 - val_loss: 0.0225 - val_accuracy: 0.9938\n",
      "Epoch 341/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0434 - accuracy: 0.9887 - val_loss: 0.0237 - val_accuracy: 0.9923\n",
      "Epoch 342/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0473 - accuracy: 0.9867 - val_loss: 0.0274 - val_accuracy: 0.9908\n",
      "Epoch 343/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0525 - accuracy: 0.9843 - val_loss: 0.0228 - val_accuracy: 0.9938\n",
      "Epoch 344/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0452 - accuracy: 0.9867 - val_loss: 0.0234 - val_accuracy: 0.9931\n",
      "Epoch 345/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0423 - accuracy: 0.9892 - val_loss: 0.0231 - val_accuracy: 0.9931\n",
      "Epoch 346/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0437 - accuracy: 0.9879 - val_loss: 0.0218 - val_accuracy: 0.9954\n",
      "Epoch 347/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0431 - accuracy: 0.9887 - val_loss: 0.0235 - val_accuracy: 0.9923\n",
      "Epoch 348/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0436 - accuracy: 0.9887 - val_loss: 0.0251 - val_accuracy: 0.9908\n",
      "Epoch 349/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0473 - accuracy: 0.9882 - val_loss: 0.0362 - val_accuracy: 0.9900\n",
      "Epoch 350/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0502 - accuracy: 0.9861 - val_loss: 0.0247 - val_accuracy: 0.9915\n",
      "Epoch 351/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0446 - accuracy: 0.9874 - val_loss: 0.0217 - val_accuracy: 0.9946\n",
      "Epoch 352/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0439 - accuracy: 0.9897 - val_loss: 0.0263 - val_accuracy: 0.9915\n",
      "Epoch 353/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0446 - accuracy: 0.9890 - val_loss: 0.0224 - val_accuracy: 0.9938\n",
      "Epoch 354/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0460 - accuracy: 0.9874 - val_loss: 0.0329 - val_accuracy: 0.9846\n",
      "Epoch 355/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0449 - accuracy: 0.9877 - val_loss: 0.0217 - val_accuracy: 0.9946\n",
      "Epoch 356/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0443 - accuracy: 0.9885 - val_loss: 0.0246 - val_accuracy: 0.9923\n",
      "Epoch 357/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0484 - accuracy: 0.9856 - val_loss: 0.0313 - val_accuracy: 0.9862\n",
      "Epoch 358/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0451 - accuracy: 0.9882 - val_loss: 0.0238 - val_accuracy: 0.9931\n",
      "Epoch 359/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0451 - accuracy: 0.9861 - val_loss: 0.0313 - val_accuracy: 0.9862\n",
      "Epoch 360/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0456 - accuracy: 0.9885 - val_loss: 0.0221 - val_accuracy: 0.9946\n",
      "Epoch 361/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0433 - accuracy: 0.9877 - val_loss: 0.0215 - val_accuracy: 0.9946\n",
      "Epoch 362/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0436 - accuracy: 0.9877 - val_loss: 0.0213 - val_accuracy: 0.9946\n",
      "Epoch 363/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0450 - accuracy: 0.9877 - val_loss: 0.0235 - val_accuracy: 0.9931\n",
      "Epoch 364/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0581 - accuracy: 0.9838 - val_loss: 0.0248 - val_accuracy: 0.9923\n",
      "Epoch 365/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0543 - accuracy: 0.9828 - val_loss: 0.0287 - val_accuracy: 0.9908\n",
      "Epoch 366/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0453 - accuracy: 0.9882 - val_loss: 0.0250 - val_accuracy: 0.9908\n",
      "Epoch 367/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0443 - accuracy: 0.9877 - val_loss: 0.0220 - val_accuracy: 0.9938\n",
      "Epoch 368/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0440 - accuracy: 0.9885 - val_loss: 0.0224 - val_accuracy: 0.9938\n",
      "Epoch 369/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0433 - accuracy: 0.9879 - val_loss: 0.0266 - val_accuracy: 0.9908\n",
      "Epoch 370/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0426 - accuracy: 0.9885 - val_loss: 0.0211 - val_accuracy: 0.9946\n",
      "Epoch 371/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0439 - accuracy: 0.9872 - val_loss: 0.0219 - val_accuracy: 0.9938\n",
      "Epoch 372/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0453 - accuracy: 0.9877 - val_loss: 0.0262 - val_accuracy: 0.9915\n",
      "Epoch 373/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0478 - accuracy: 0.9859 - val_loss: 0.0263 - val_accuracy: 0.9915\n",
      "Epoch 374/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0448 - accuracy: 0.9877 - val_loss: 0.0278 - val_accuracy: 0.9908\n",
      "Epoch 375/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0432 - accuracy: 0.9877 - val_loss: 0.0220 - val_accuracy: 0.9938\n",
      "Epoch 376/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0424 - accuracy: 0.9887 - val_loss: 0.0225 - val_accuracy: 0.9931\n",
      "Epoch 377/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0423 - accuracy: 0.9885 - val_loss: 0.0206 - val_accuracy: 0.9946\n",
      "Epoch 378/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0417 - accuracy: 0.9892 - val_loss: 0.0210 - val_accuracy: 0.9946\n",
      "Epoch 379/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0420 - accuracy: 0.9897 - val_loss: 0.0231 - val_accuracy: 0.9938\n",
      "Epoch 380/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0474 - accuracy: 0.9869 - val_loss: 0.0233 - val_accuracy: 0.9938\n",
      "Epoch 381/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0431 - accuracy: 0.9877 - val_loss: 0.0209 - val_accuracy: 0.9954\n",
      "Epoch 382/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0420 - accuracy: 0.9885 - val_loss: 0.0239 - val_accuracy: 0.9915\n",
      "Epoch 383/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0477 - accuracy: 0.9867 - val_loss: 0.0288 - val_accuracy: 0.9892\n",
      "Epoch 384/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0476 - accuracy: 0.9874 - val_loss: 0.0256 - val_accuracy: 0.9915\n",
      "Epoch 385/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0446 - accuracy: 0.9872 - val_loss: 0.0240 - val_accuracy: 0.9931\n",
      "Epoch 386/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0432 - accuracy: 0.9890 - val_loss: 0.0206 - val_accuracy: 0.9946\n",
      "Epoch 387/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0494 - accuracy: 0.9851 - val_loss: 0.0211 - val_accuracy: 0.9946\n",
      "Epoch 388/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0476 - accuracy: 0.9874 - val_loss: 0.0231 - val_accuracy: 0.9931\n",
      "Epoch 389/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0442 - accuracy: 0.9869 - val_loss: 0.0222 - val_accuracy: 0.9938\n",
      "Epoch 390/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0413 - accuracy: 0.9887 - val_loss: 0.0206 - val_accuracy: 0.9938\n",
      "Epoch 391/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0419 - accuracy: 0.9895 - val_loss: 0.0199 - val_accuracy: 0.9954\n",
      "Epoch 392/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0438 - accuracy: 0.9887 - val_loss: 0.0206 - val_accuracy: 0.9946\n",
      "Epoch 393/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0429 - accuracy: 0.9872 - val_loss: 0.0236 - val_accuracy: 0.9923\n",
      "Epoch 394/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0424 - accuracy: 0.9885 - val_loss: 0.0236 - val_accuracy: 0.9915\n",
      "Epoch 395/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0421 - accuracy: 0.9887 - val_loss: 0.0237 - val_accuracy: 0.9915\n",
      "Epoch 396/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0445 - accuracy: 0.9877 - val_loss: 0.0221 - val_accuracy: 0.9946\n",
      "Epoch 397/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0419 - accuracy: 0.9890 - val_loss: 0.0214 - val_accuracy: 0.9938\n",
      "Epoch 398/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0422 - accuracy: 0.9892 - val_loss: 0.0211 - val_accuracy: 0.9946\n",
      "Epoch 399/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0426 - accuracy: 0.9897 - val_loss: 0.0223 - val_accuracy: 0.9938\n",
      "Epoch 400/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0495 - accuracy: 0.9867 - val_loss: 0.0210 - val_accuracy: 0.9946\n",
      "Epoch 401/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0456 - accuracy: 0.9869 - val_loss: 0.0213 - val_accuracy: 0.9946\n",
      "Epoch 402/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0426 - accuracy: 0.9895 - val_loss: 0.0206 - val_accuracy: 0.9954\n",
      "Epoch 403/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0443 - accuracy: 0.9885 - val_loss: 0.0263 - val_accuracy: 0.9915\n",
      "Epoch 404/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0494 - accuracy: 0.9846 - val_loss: 0.0243 - val_accuracy: 0.9931\n",
      "Epoch 405/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0448 - accuracy: 0.9872 - val_loss: 0.0213 - val_accuracy: 0.9938\n",
      "Epoch 406/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0448 - accuracy: 0.9867 - val_loss: 0.0231 - val_accuracy: 0.9915\n",
      "Epoch 407/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0416 - accuracy: 0.9895 - val_loss: 0.0231 - val_accuracy: 0.9915\n",
      "Epoch 408/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0420 - accuracy: 0.9895 - val_loss: 0.0197 - val_accuracy: 0.9946\n",
      "Epoch 409/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0425 - accuracy: 0.9885 - val_loss: 0.0209 - val_accuracy: 0.9931\n",
      "Epoch 410/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0421 - accuracy: 0.9887 - val_loss: 0.0209 - val_accuracy: 0.9938\n",
      "Epoch 411/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0413 - accuracy: 0.9892 - val_loss: 0.0204 - val_accuracy: 0.9946\n",
      "Epoch 412/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0417 - accuracy: 0.9887 - val_loss: 0.0213 - val_accuracy: 0.9938\n",
      "Epoch 413/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0412 - accuracy: 0.9897 - val_loss: 0.0206 - val_accuracy: 0.9938\n",
      "Epoch 414/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0411 - accuracy: 0.9887 - val_loss: 0.0203 - val_accuracy: 0.9946\n",
      "Epoch 415/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0440 - accuracy: 0.9879 - val_loss: 0.0216 - val_accuracy: 0.9931\n",
      "Epoch 416/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0532 - accuracy: 0.9846 - val_loss: 0.0207 - val_accuracy: 0.9938\n",
      "Epoch 417/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0464 - accuracy: 0.9856 - val_loss: 0.0235 - val_accuracy: 0.9915\n",
      "Epoch 418/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0473 - accuracy: 0.9872 - val_loss: 0.0232 - val_accuracy: 0.9931\n",
      "Epoch 419/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0446 - accuracy: 0.9869 - val_loss: 0.0232 - val_accuracy: 0.9931\n",
      "Epoch 420/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0440 - accuracy: 0.9879 - val_loss: 0.0246 - val_accuracy: 0.9923\n",
      "Epoch 421/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0428 - accuracy: 0.9895 - val_loss: 0.0258 - val_accuracy: 0.9923\n",
      "Epoch 422/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0446 - accuracy: 0.9879 - val_loss: 0.0207 - val_accuracy: 0.9938\n",
      "Epoch 423/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0435 - accuracy: 0.9887 - val_loss: 0.0204 - val_accuracy: 0.9938\n",
      "Epoch 424/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0414 - accuracy: 0.9890 - val_loss: 0.0201 - val_accuracy: 0.9946\n",
      "Epoch 425/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0414 - accuracy: 0.9892 - val_loss: 0.0205 - val_accuracy: 0.9931\n",
      "Epoch 426/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0409 - accuracy: 0.9887 - val_loss: 0.0207 - val_accuracy: 0.9938\n",
      "Epoch 427/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0406 - accuracy: 0.9890 - val_loss: 0.0209 - val_accuracy: 0.9946\n",
      "Epoch 428/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0427 - accuracy: 0.9882 - val_loss: 0.0217 - val_accuracy: 0.9946\n"
     ]
    }
   ],
   "source": [
    "# 학습이 언제 자동 중단 될지를 설정합니다.\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "#최적화 모델이 저장될 폴더와 모델의 이름을 정합니다.\n",
    "modelpath=\"./data/model/Ch14-4-bestmodel.hdf5\"\n",
    "\n",
    "# 최적화 모델을 업데이트하고 저장합니다.\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "#모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, validation_split=0.25, verbose=1, callbacks=[early_stopping_callback,checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 6ms/step - loss: 0.0606 - accuracy: 0.9846\n",
      "Test accuracy: 0.9846153855323792\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
